# Hadoop
## Hadoop이란?
```
분산 환경에서 빅데이터를 저장하고 처리할 수 있는 자바 기반의 오픈 소스 프레임 워크
```
단일 서버에 수천대의 머신으로 확장할 수 있도록 설계되었다.  
일반적으로 하둡파일시스템(HDFS)과 맵리듀스(MapReduce)프레임워크로 시작되었으니, 여러 데이터 저장, 실행 엔진, 프로그래밍 및 데이터 처리와 같은 `하둡 생태계(Haddoop Ecosystem)` 전반을 포함하는 의미로 확장, 발전되었다.

![img](https://user-images.githubusercontent.com/59672592/186151156-c2a06077-50f3-41a4-abcf-d9646c144517.png)

- 하둡 코어 프로젝트 : HDFS(분산 데이터 저장), MapReduce(분산 처리)
- 하둡 서브 프로젝트 : 나머지 프로젝트들 -> 데이터 마이닝, 수집, 분석 등을 수행한다.

## 하둡 분산형 파일 시스템(Hadoop Distributed File System, HDFS)
```
하둡 네트워크에 연결된 기기에 데이터를 저장하는 분산형 파일 시스템
```
하둡 분산 파일 시스템은 하둡 프레임워크를 위해 자바 언어로 작성된 분산 확장 파일 시스템이다. HDFS는 여러 기계에 대용량 파일을 나눠서 저장을 한다. 데이터들을 여러 서버에 중복해서 저장함으로써 데이터 안정성을 얻는다. 
### 특징
- HDFS는 데이터를 저장하면, 다수의 노드에 복제 데이터도 함께 저장해서 데이터 유실을 방지한다.
- HDFS에 파일을 저장하거나, 저장된 파일을 조회하려면 스트리밍 방식으로 데이터에 접근해야한다.
- 한번 저장한 데이터는 수정할 수 없고, 읽기만 가능해서 데이터 무결성을 유지한다. 
- 데이터 수정은 불가능하지만 파일 이동, 삭제, 복사할 수 있는 인터페이스르 제공한다.

## 파일 저장 플로우

![파일 저장 플로우](https://user-images.githubusercontent.com/59672592/186154351-87d77e52-0acf-42f8-9da6-3bd134be99d4.png)
1. 어플리케이션이 HDFS 클라이언트에게 파일 저장을 요청하면, 
클라이언트는 네임 노드에게 파일 블록들이 저장될 경로 생성 요청  
네임 노드는 해당 파일 경로가 존재하지 않으면 경로 생성 후 다른 클라이언트가 해당 경로를 수정하지 못하게 lock  
이후 네임노드는 클라이언트에게 해당 파일 블록들을 저장할 데이터 노드의 목록 반환
2. 클라이언트는 첫번째 데이터 노드에게 데이터 전송
3. 첫번째 데이터 노드는 데이터를 로컬에 저장한 후 데이터를 두번째 데이터 노드로 전송. 이후 계속 전달
4. 로컬에 데이터를 저장하였면 자기에게 넘겨준 데이터 노드에게 데이터 로컬 저장이 완료되었음을 응답.(ack)
5. 첫번째 데이터 노드는 클라이언트에게 데이터 저장이 완료되었음을 응답.

# MapReduce
## MapReduce란?
```
 대용량의 데이터를 분산/병렬 컴퓨팅 환경에서 처리하기 위해 제작된 데이터 처리 모델
```
- 큰 데이터가 들어왔을 때, 데이터를 특정 크기의 블록으로 나누고 각 블록에 대해 Map Task와 Reduce Task를 수행
- Map Task와 Reduce Task는 입력과 출력으로 Key-Value 구조를 사용
-  Map은 처리한 데이터를 (Key, Value)의 형태로 묶는 작업을 의미
- Map에서는 묶은 (Key, Value) 형태의 데이터들을 List의 형태로 반환
- Reduce는 Map으로 처리한 데이터에서 중복된 key 값을 지니는 데이터를 제거하여 합치고, 원하는 데이터를 추출하는 작업을 수행
##  MapReduce 처리 과정
![24_mapreduce (1)](https://user-images.githubusercontent.com/59672592/186157123-210e466c-1283-47c4-b171-33571278c53d.png)
1. 단어의 개수를 세기 위한 텍스트 파일들을 HDFS에 업로드하고, 각각의 파일은 블록단위로 나누어 저장
2. Spitting 과정을 통해 블록 안의 텍스트 파일을 한 줄로 분할
3. line을 공백 기준으로 분리하고, Map(line_num, line) 연산을 통해 (vocabulary, 1개)의 리스트를 반환
4. Shuffling 과정을 통해 연관성있는 데이터들끼리 모아 정렬
5. Reduce(단어, 개수)를 수행하여 각 블록에서 특정 단어가 몇번 나왔는지를 계산
6. 이후에 결과를 합산하여 HDFS에 파일로 결과를 저장